{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Supreme\n",
      "[nltk_data]     Leader\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import html\n",
    "import pprint\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.stem.snowball import *\n",
    "from html.parser import HTMLParser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import*\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.extend(['Reuter','zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Class which contains methods to parse the SGML Files\n",
    "\n",
    "\n",
    "class ReutersParser(HTMLParser):\n",
    "    \"\"\"\n",
    "    ReutersParser subclasses HTMLParser and is used to open the SGML\n",
    "    files associated with the Reuters-21578 categorised test collection.\n",
    "\n",
    "    The parser is a generator and will yield a single document at a time.\n",
    "    Since the data will be chunked on parsing, it is necessary to keep \n",
    "    some internal state of when tags have been \"entered\" and \"exited\".\n",
    "    Hence the in_body, in_topics and in_topic_d boolean members.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoding='latin-1'):\n",
    "        \"\"\"\n",
    "        Initialise the superclass (HTMLParser) and reset the parser.\n",
    "        Sets the encoding of the SGML files by default to latin-1.\n",
    "        \"\"\"\n",
    "        html.parser.HTMLParser.__init__(self)\n",
    "        self._reset()\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "        This is called only on initialisation of the parser class\n",
    "        and when a new topic-body tuple has been generated. It\n",
    "        resets all off the state so that a new tuple can be subsequently\n",
    "        generated.\n",
    "        \"\"\"\n",
    "        self.in_body = False\n",
    "        self.in_topics = False\n",
    "        self.in_topic_d = False\n",
    "        self.body = \"\"\n",
    "        self.topics = []\n",
    "        self.topic_d = \"\"\n",
    "\n",
    "    def parse(self, fd):\n",
    "        \"\"\"\n",
    "        parse accepts a file descriptor and loads the data in chunks\n",
    "        in order to minimise memory usage. It then yields new documents\n",
    "        as they are parsed.\n",
    "        \"\"\"\n",
    "        self.docs = []\n",
    "        for chunk in fd:\n",
    "            self.feed(chunk.decode(self.encoding))\n",
    "            for doc in self.docs:\n",
    "                yield doc\n",
    "            self.docs = []\n",
    "        self.close()\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        \"\"\"\n",
    "        This method is used to determine what to do when the parser\n",
    "        comes across a particular tag of type \"tag\". In this instance\n",
    "        we simply set the internal state booleans to True if that particular\n",
    "        tag has been found.\n",
    "        \"\"\"\n",
    "        if tag == \"reuters\":\n",
    "            pass\n",
    "        elif tag == \"body\":\n",
    "            self.in_body = True\n",
    "        elif tag == \"topics\":\n",
    "            self.in_topics = True\n",
    "        elif tag == \"d\":\n",
    "            self.in_topic_d = True \n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        \"\"\"\n",
    "        This method is used to determine what to do when the parser\n",
    "        finishes with a particular tag of type \"tag\". \n",
    "\n",
    "        If the tag is a <REUTERS> tag, then we remove all \n",
    "        white-space with a regular expression and then append the \n",
    "        topic-body tuple.\n",
    "\n",
    "        If the tag is a <BODY> or <TOPICS> tag then we simply set\n",
    "        the internal state to False for these booleans, respectively.\n",
    "\n",
    "        If the tag is a <D> tag (found within a <TOPICS> tag), then we\n",
    "        append the particular topic to the \"topics\" list and \n",
    "        finally reset it.\n",
    "        \"\"\"\n",
    "        if tag == \"reuters\":\n",
    "            self.body = re.sub(r'\\s+', r' ', self.body)\n",
    "            self.docs.append( (self.topics, self.body) )\n",
    "            self._reset()\n",
    "        elif tag == \"body\":\n",
    "            self.in_body = False\n",
    "        elif tag == \"topics\":\n",
    "            self.in_topics = False\n",
    "        elif tag == \"d\":\n",
    "            self.in_topic_d = False\n",
    "            self.topics.append(self.topic_d)\n",
    "            self.topic_d = \"\"  \n",
    "\n",
    "    def handle_data(self, data):\n",
    "        \"\"\"\n",
    "        The data is simply appended to the appropriate member state\n",
    "        for that particular tag, up until the end closing tag appears.\n",
    "        \"\"\"\n",
    "        if self.in_body:\n",
    "            self.body += data\n",
    "        elif self.in_topic_d:\n",
    "            self.topic_d += data\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the list of Reuters data and create the parser\n",
    "    files = [r\"C:\\Users\\Supreme Leader\\Desktop\\DMML 2/reut2-%03d.sgm\" % r for r in range(0, 22)]\n",
    "    parser = ReutersParser()\n",
    "\n",
    "    # Parse the document and force all generated docs into\n",
    "    # a list so that it can be printed out to the console\n",
    "    docs = []\n",
    "    for fn in files:\n",
    "        for d in parser.parse(open(fn, 'rb')):\n",
    "            docs.append(d)\n",
    "            \n",
    "def obtain_topic_tags():\n",
    "    \n",
    "#    Open the topic list file and import all of the topic names\n",
    "#    taking care to strip the trailing \"\\n\" from each word.\n",
    "    \n",
    "    \n",
    "    topics = open(r\"C:\\Users\\Supreme Leader\\Desktop\\DMML 2\\all-topics-strings.lc.txt\").readlines()\n",
    "    topics = [t.strip() for t in topics]\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Reads all of the documents and creates a new list of two-tuples\n",
    "#  that contains only the topic list and the body text.\n",
    "#  It removes all geographic features and only \n",
    "#  retains those documents which have at least one non-geographic\n",
    "#  topic.\n",
    "    \n",
    "\n",
    "def filter_doc_list_through_topics(topics, docs):\n",
    "    \n",
    "           \n",
    "    ref_docs = []\n",
    "    for d in docs:\n",
    "        \n",
    "        if d[0] == [] or d[0]== \"\":\n",
    "            \n",
    "            continue\n",
    "        a = []\n",
    "        for t in d[0]:\n",
    "          \n",
    "            \n",
    "            if t in topics:\n",
    "                \n",
    "                \n",
    "                a.append(t)\n",
    "        if a!=[] and d[1]!= \"\":\n",
    "            \n",
    "            \n",
    "            ref_docs.append((a,d[1]))\n",
    "            \n",
    "                \n",
    "    return ref_docs\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics= obtain_topic_tags() # Obtain all topic tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_docs = filter_doc_list_through_topics(topics, docs)# Strip all other tags excluding those in topic list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ref_docs, columns = ['Label', 'Text']) # Convert the data into a dataframe for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming the articles word by word. eg- amusement, amusing is all truncated to amuse\n",
    "t = []\n",
    "for i,r in df.iterrows():\n",
    "    \n",
    "    t.append(re.findall(r'\\w+', r['Text']))\n",
    "    \n",
    "stemmer = SnowballStemmer('english')\n",
    "j = []\n",
    "for k in t:    \n",
    "    j.append(' '.join([stemmer.stem(x) for x in k]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the stemmed articles back into the data frame\n",
    "df['Text1'] = j\n",
    "df.drop('Text', axis = 1, inplace = True)\n",
    "df.columns = ['Label','Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We format the trainign data alone that an article has one corresponding topic, in a data frame. In case an article has multiple \n",
    "# topics assigned to it, say (d,c1,c2,c3)  we convert it into multiple rows (d,c1),(d,c2),(d,c3) \n",
    "\n",
    "def FormatTraindata(X_train,y_train):\n",
    "       \n",
    "    \n",
    "    \n",
    "    x=pd.concat([X_train,y_train],axis=1)\n",
    "    Q = pd.DataFrame()\n",
    "    Class =[]\n",
    "    Article=[]\n",
    "    \n",
    "    for i,r in x.iterrows():\n",
    "        \n",
    "        \n",
    "        if len(r['Label'])==1:\n",
    "            \n",
    "            \n",
    "            Class.append(r['Label'][0])\n",
    "            Article.append(r['Text'])\n",
    "        elif len(r['Label'])>1:\n",
    "            \n",
    "                    \n",
    "            for j in r['Label']:\n",
    "                \n",
    "                Article.append(r['Text'])\n",
    "                Class.append(j)\n",
    "    \n",
    "    Q['Text']=Article\n",
    "    Q['Label']=Class\n",
    "    \n",
    "    return(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping Labels back to the predicted data after applying a threshold\n",
    "\n",
    "def thresholdcomp(prediction,T,thr):\n",
    "    ans = []\n",
    "    for i in prediction:\n",
    "        n = []\n",
    "        for j in range(len(prediction[0])):\n",
    "            if i[j] > thr:\n",
    "                n.append(T[j])\n",
    "        ans.append(n)\n",
    "    return(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Jacard index, a measure of precision( True positive/ True Positive + False Negative + False Positive)  \n",
    "\n",
    "def Jacardscr(ans,y_test):\n",
    "    \n",
    "    js = 0\n",
    "    \n",
    "    for i in range(len(ans)):\n",
    "        js += len(set(list(y_test)[i]) & set(ans[i]))/len(set(list(y_test)[i]) | set(ans[i]))\n",
    "    \n",
    "    js=js/len(ans)\n",
    "    return(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K fold cross validation on the data where k is a user defined parameter. \n",
    "# In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples.\n",
    "# Of the k subsamples, a single subsample is retained as the validation data for testing the model, \n",
    "# and the remaining k − 1 subsamples are used as training data. \n",
    "\n",
    "# The initial data which is processed into a data frame with two columns : Articles(Data points) and Labels(Classes)\n",
    "# is also an argument to the funtion.\n",
    "\n",
    "# The Probabilistic threshold 'thr' for predicting class labels is passed as an argument as well\n",
    "\n",
    "\n",
    "def kfold(df,k,thr):\n",
    "    kf = KFold(k,shuffle = True)\n",
    "    kf.get_n_splits(df['Text'])\n",
    "    \n",
    "# Split the data into training data and test data\n",
    "    \n",
    "    Out =[]\n",
    "    i=0\n",
    "    \n",
    "    for trainindex,testindex in kf.split(df['Text']):\n",
    "        print ('Train:', trainindex,'Test:',testindex)\n",
    "        X_train,X_test = df['Text'].iloc[trainindex], df['Text'].iloc[testindex]\n",
    "        y_train,y_test = df['Label'].iloc[trainindex], df['Label'].iloc[testindex]\n",
    "\n",
    "        Q = FormatTraindata(X_train,y_train)\n",
    "        T= sorted(set(Q['Label']))\n",
    "        \n",
    "        # We sort the labels in alphabetical order so as to later map it back to the predicted test\n",
    "        #data based on indices\n",
    "\n",
    "        \n",
    "        \n",
    "        # One vs rest strategy involves training a single classifier per class, with the samples of that class as \n",
    "        # positive samples and all other samples as negatives. This strategy requires the \n",
    "        # base classifiers to produce a real-valued confidence score for its decision, rather than just a class label;\n",
    "        # discrete class labels alone can lead to ambiguities, where multiple classes are predicted for a single sample\n",
    "        \n",
    "        # Define a pipeline combining a text feature extractor with multi label classifier(One vs rest)\n",
    "        # pipeline allows performing a sequence of different transformations\n",
    "        \n",
    "        NB_pipeline = Pipeline([\n",
    "                    ('tfidf', TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2',stop_words = stopwords)),\n",
    "                    ('clf', OneVsRestClassifier(MultinomialNB(alpha=0,\n",
    "                        fit_prior=True, class_prior=None))),\n",
    "                ])\n",
    "\n",
    "        NB_pipeline.fit(Q['Text'],Q['Label']) # Fit the pipleline  to the training data\n",
    "\n",
    "        prediction = NB_pipeline.predict_proba(X_test) # Predict the probabilities associated with each class for an article,\n",
    "                                                       # output as an array\n",
    "\n",
    "        ans=thresholdcomp(prediction,T,thr)     # Map the Labels to the articles based on a probability threshold.\n",
    "                                    #       ie . the classes/labels are assigned if the probability ofhaving  a class associated with\n",
    "                                            # an article is higher than a specified threshold\n",
    "        \n",
    "        h= pd.DataFrame()\n",
    "        h['Text'] = X_test\n",
    "        h[\"Actual lab\"] = y_test\n",
    "        h[\"Predicted\"] = ans\n",
    "        Out.append(h)\n",
    "        \n",
    "        print(\"len = \",len(ans))\n",
    "\n",
    "        js=Jacardscr(ans,y_test)    # Compute the Jacard index\n",
    "\n",
    "        print (js) # The Jacard index, a measure of precision of prediction is output\n",
    "        \n",
    "        h.to_csv(\"C:/Users/Supreme Leader/Desktop/Output-threshold = \"+\" \" +str(thr)+\" \"+\" Jacscore = \"+\" \"+str(js)+\" \" +str(i)+\".csv\",sep='\\t')\n",
    "        \n",
    "        i=i+1    \n",
    "        \n",
    "        \n",
    "    return(Out) # Returns a list with each element of the output , a data frame which contains the article, actual label \n",
    "                # and the predicted label. The list has k items as we perform a kfold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [    0     2     3 ... 10372 10373 10376] Test: [    1     7    13 ... 10366 10374 10375]\n",
      "len =  1483\n",
      "0.8330553390566877\n",
      "Train: [    0     1     2 ... 10374 10375 10376] Test: [    6     8    10 ... 10358 10359 10360]\n",
      "len =  1483\n",
      "0.8236200751372702\n",
      "Train: [    0     1     2 ... 10373 10374 10375] Test: [    4    14    32 ... 10367 10370 10376]\n",
      "len =  1483\n",
      "0.8261945142592475\n",
      "Train: [    1     2     4 ... 10374 10375 10376] Test: [    0     3     5 ... 10344 10357 10368]\n",
      "len =  1482\n",
      "0.8236230265835528\n",
      "Train: [    0     1     3 ... 10374 10375 10376] Test: [    2    11    31 ... 10364 10371 10373]\n",
      "len =  1482\n",
      "0.8302537861748387\n",
      "Train: [    0     1     2 ... 10374 10375 10376] Test: [   12    24    25 ... 10361 10365 10369]\n",
      "len =  1482\n",
      "0.8434483645009957\n",
      "Train: [    0     1     2 ... 10374 10375 10376] Test: [    9    19    28 ... 10338 10347 10372]\n",
      "len =  1482\n",
      "0.8152641218430695\n"
     ]
    }
   ],
   "source": [
    "Out = kfold(df,7,0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
